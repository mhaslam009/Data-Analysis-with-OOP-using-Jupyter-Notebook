{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a80e2a2",
   "metadata": {},
   "source": [
    "<h1>Pandas class3: aggregating and grouped summary</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4702c7a0",
   "metadata": {},
   "source": [
    "<h2>Aggregating Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5236e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1197589a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                region                 state  individuals  family_members  \\\n",
      "0   East South Central               Alabama       2570.0           864.0   \n",
      "1              Pacific                Alaska       1434.0           582.0   \n",
      "2             Mountain               Arizona       7259.0          2606.0   \n",
      "3   West South Central              Arkansas       2280.0           432.0   \n",
      "4              Pacific            California     109008.0         20964.0   \n",
      "5             Mountain              Colorado       7607.0          3250.0   \n",
      "6          New England           Connecticut       2280.0          1696.0   \n",
      "7       South Atlantic              Delaware        708.0           374.0   \n",
      "8       South Atlantic  District of Columbia       3770.0          3134.0   \n",
      "9       South Atlantic               Florida      21443.0          9587.0   \n",
      "10      South Atlantic               Georgia       6943.0          2556.0   \n",
      "11             Pacific                Hawaii       4131.0          2399.0   \n",
      "12            Mountain                 Idaho       1297.0           715.0   \n",
      "13  East North Central              Illinois       6752.0          3891.0   \n",
      "14  East North Central               Indiana       3776.0          1482.0   \n",
      "15  West North Central                  Iowa       1711.0          1038.0   \n",
      "16  West North Central                Kansas       1443.0           773.0   \n",
      "17  East South Central              Kentucky       2735.0           953.0   \n",
      "18  West South Central             Louisiana       2540.0           519.0   \n",
      "19         New England                 Maine       1450.0          1066.0   \n",
      "20      South Atlantic              Maryland       4914.0          2230.0   \n",
      "21         New England         Massachusetts       6811.0         13257.0   \n",
      "22  East North Central              Michigan       5209.0          3142.0   \n",
      "23  West North Central             Minnesota       3993.0          3250.0   \n",
      "24  East South Central           Mississippi       1024.0           328.0   \n",
      "25  West North Central              Missouri       3776.0          2107.0   \n",
      "26            Mountain               Montana        983.0           422.0   \n",
      "27  West North Central              Nebraska       1745.0           676.0   \n",
      "28            Mountain                Nevada       7058.0           486.0   \n",
      "29         New England         New Hampshire        835.0           615.0   \n",
      "30        Mid-Atlantic            New Jersey       6048.0          3350.0   \n",
      "31            Mountain            New Mexico       1949.0           602.0   \n",
      "32        Mid-Atlantic              New York      39827.0         52070.0   \n",
      "33      South Atlantic        North Carolina       6451.0          2817.0   \n",
      "34  West North Central          North Dakota        467.0            75.0   \n",
      "35  East North Central                  Ohio       6929.0          3320.0   \n",
      "36  West South Central              Oklahoma       2823.0          1048.0   \n",
      "37             Pacific                Oregon      11139.0          3337.0   \n",
      "38        Mid-Atlantic          Pennsylvania       8163.0          5349.0   \n",
      "39         New England          Rhode Island        747.0           354.0   \n",
      "40      South Atlantic        South Carolina       3082.0           851.0   \n",
      "41  West North Central          South Dakota        836.0           323.0   \n",
      "42  East South Central             Tennessee       6139.0          1744.0   \n",
      "43  West South Central                 Texas      19199.0          6111.0   \n",
      "44            Mountain                  Utah       1904.0           972.0   \n",
      "45         New England               Vermont        780.0           511.0   \n",
      "46      South Atlantic              Virginia       3928.0          2047.0   \n",
      "47             Pacific            Washington      16424.0          5880.0   \n",
      "48      South Atlantic         West Virginia       1021.0           222.0   \n",
      "49  East North Central             Wisconsin       2740.0          2167.0   \n",
      "50            Mountain               Wyoming        434.0           205.0   \n",
      "\n",
      "    state_pop  \n",
      "0     4887681  \n",
      "1      735139  \n",
      "2     7158024  \n",
      "3     3009733  \n",
      "4    39461588  \n",
      "5     5691287  \n",
      "6     3571520  \n",
      "7      965479  \n",
      "8      701547  \n",
      "9    21244317  \n",
      "10   10511131  \n",
      "11    1420593  \n",
      "12    1750536  \n",
      "13   12723071  \n",
      "14    6695497  \n",
      "15    3148618  \n",
      "16    2911359  \n",
      "17    4461153  \n",
      "18    4659690  \n",
      "19    1339057  \n",
      "20    6035802  \n",
      "21    6882635  \n",
      "22    9984072  \n",
      "23    5606249  \n",
      "24    2981020  \n",
      "25    6121623  \n",
      "26    1060665  \n",
      "27    1925614  \n",
      "28    3027341  \n",
      "29    1353465  \n",
      "30    8886025  \n",
      "31    2092741  \n",
      "32   19530351  \n",
      "33   10381615  \n",
      "34     758080  \n",
      "35   11676341  \n",
      "36    3940235  \n",
      "37    4181886  \n",
      "38   12800922  \n",
      "39    1058287  \n",
      "40    5084156  \n",
      "41     878698  \n",
      "42    6771631  \n",
      "43   28628666  \n",
      "44    3153550  \n",
      "45     624358  \n",
      "46    8501286  \n",
      "47    7523869  \n",
      "48    1804291  \n",
      "49    5807406  \n",
      "50     577601  \n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "       store type  department        date  weekly_sales  is_holiday  \\\n",
      "0          1    A           1  2010-02-05      24924.50       False   \n",
      "1          1    A           1  2010-03-05      21827.90       False   \n",
      "2          1    A           1  2010-04-02      57258.43       False   \n",
      "3          1    A           1  2010-05-07      17413.94       False   \n",
      "4          1    A           1  2010-06-04      17558.09       False   \n",
      "...      ...  ...         ...         ...           ...         ...   \n",
      "10769     39    A          99  2011-12-09        895.00       False   \n",
      "10770     39    A          99  2012-02-03        350.00       False   \n",
      "10771     39    A          99  2012-06-08        450.00       False   \n",
      "10772     39    A          99  2012-07-13          0.06       False   \n",
      "10773     39    A          99  2012-10-05        915.00       False   \n",
      "\n",
      "       temperature_c  fuel_price_usd_per_l  unemployment  \n",
      "0           5.727778              0.679451         8.106  \n",
      "1           8.055556              0.693452         8.106  \n",
      "2          16.816667              0.718284         7.808  \n",
      "3          22.527778              0.748928         7.808  \n",
      "4          27.050000              0.714586         7.808  \n",
      "...              ...                   ...           ...  \n",
      "10769       9.644444              0.834256         7.716  \n",
      "10770      15.938889              0.887619         7.244  \n",
      "10771      27.288889              0.911922         6.989  \n",
      "10772      25.644444              0.860145         6.623  \n",
      "10773      22.250000              0.955511         6.228  \n",
      "\n",
      "[10774 rows x 9 columns]\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "             date     city        country  avg_temp_c\n",
      "0      2000-01-01  Abidjan  Côte D'Ivoire      27.293\n",
      "1      2000-02-01  Abidjan  Côte D'Ivoire      27.685\n",
      "2      2000-03-01  Abidjan  Côte D'Ivoire      29.061\n",
      "3      2000-04-01  Abidjan  Côte D'Ivoire      28.162\n",
      "4      2000-05-01  Abidjan  Côte D'Ivoire      27.547\n",
      "...           ...      ...            ...         ...\n",
      "16495  2013-05-01     Xian          China      18.979\n",
      "16496  2013-06-01     Xian          China      23.522\n",
      "16497  2013-07-01     Xian          China      25.251\n",
      "16498  2013-08-01     Xian          China      24.528\n",
      "16499  2013-09-01     Xian          China         NaN\n",
      "\n",
      "[16500 rows x 4 columns]\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "         country    capital    area  population\n",
      "BR        Brazil   Brasilia   8.516      200.40\n",
      "RU        Russia     Moscow  17.100      143.50\n",
      "IN         India  New Delhi   3.286     1252.00\n",
      "CH         China    Beijing   9.597     1357.00\n",
      "SA  South Africa   Pretoria   1.221       52.98\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "            date          type  year  avg_price         size     nb_sold\n",
      "0     2015-12-27  conventional  2015       0.95        small  9626901.09\n",
      "1     2015-12-20  conventional  2015       0.98        small  8710021.76\n",
      "2     2015-12-13  conventional  2015       0.93        small  9855053.66\n",
      "3     2015-12-06  conventional  2015       0.89        small  9405464.36\n",
      "4     2015-11-29  conventional  2015       0.99        small  8094803.56\n",
      "...          ...           ...   ...        ...          ...         ...\n",
      "1009  2018-02-04       organic  2018       1.53  extra_large     1703.52\n",
      "1010  2018-01-28       organic  2018       1.61  extra_large     1270.61\n",
      "1011  2018-01-21       organic  2018       1.63  extra_large     1490.02\n",
      "1012  2018-01-14       organic  2018       1.59  extra_large     1580.01\n",
      "1013  2018-01-07       organic  2018       1.51  extra_large     1289.07\n",
      "\n",
      "[1014 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "homelessness_df=pd.read_csv(\"E:\\CAREER_DATA\\data_files\\homelessness.csv\",index_col=0)\n",
    "print(homelessness_df)\n",
    "print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "sales_subset_df=pd.read_csv(\"E:\\CAREER_DATA\\data_files\\\\sales_subset.csv\",index_col=0)\n",
    "print(sales_subset_df)\n",
    "print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "temperature_df=pd.read_csv(\"E:\\CAREER_DATA\\data_files\\\\temperatures.csv\",index_col=0)\n",
    "print(temperature_df)\n",
    "print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "brics_df=pd.read_csv(\"E:\\CAREER_DATA\\data_files\\\\brics.txt\",index_col=0)\n",
    "print(brics_df)\n",
    "print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "avocado_df=pd.read_pickle(\"E:\\CAREER_DATA\\data_files\\\\avoplotto.pkl\")\n",
    "print(avocado_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4725cd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['region', 'state', 'individuals', 'family_members', 'state_pop'], dtype='object')\n",
      "region             object\n",
      "state              object\n",
      "individuals       float64\n",
      "family_members    float64\n",
      "state_pop           int64\n",
      "dtype: object\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "individuals       7.225784e+03\n",
      "family_members    3.504882e+03\n",
      "state_pop         6.405637e+06\n",
      "dtype: float64\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "individuals          3082.0\n",
      "family_members       1482.0\n",
      "state_pop         4461153.0\n",
      "dtype: float64\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "individuals          434.0\n",
      "family_members        75.0\n",
      "state_pop         577601.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(homelessness_df.columns)\n",
    "print(homelessness_df.dtypes)\n",
    "print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "#mean of homelessness numerical columns\n",
    "#print(homelessness_df.mean()) #it is raising future error to resolve this i will select numeric column only\n",
    "numeric_homelessness=homelessness_df.select_dtypes(include=\"number\")\n",
    "print(numeric_homelessness.mean())\n",
    "print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "print(numeric_homelessness.median())\n",
    "print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "print(numeric_homelessness.min())\n",
    "#in the same way calculate max,var,std,sum,quantile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc431d7",
   "metadata": {},
   "source": [
    "<h1>Summary Statistics<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5b244a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               region  \\\n",
      "0                                  East South Central   \n",
      "1                           East South CentralPacific   \n",
      "2                   East South CentralPacificMountain   \n",
      "3   East South CentralPacificMountainWest South Ce...   \n",
      "4   East South CentralPacificMountainWest South Ce...   \n",
      "5   East South CentralPacificMountainWest South Ce...   \n",
      "6   East South CentralPacificMountainWest South Ce...   \n",
      "7   East South CentralPacificMountainWest South Ce...   \n",
      "8   East South CentralPacificMountainWest South Ce...   \n",
      "9   East South CentralPacificMountainWest South Ce...   \n",
      "10  East South CentralPacificMountainWest South Ce...   \n",
      "11  East South CentralPacificMountainWest South Ce...   \n",
      "12  East South CentralPacificMountainWest South Ce...   \n",
      "13  East South CentralPacificMountainWest South Ce...   \n",
      "14  East South CentralPacificMountainWest South Ce...   \n",
      "15  East South CentralPacificMountainWest South Ce...   \n",
      "16  East South CentralPacificMountainWest South Ce...   \n",
      "17  East South CentralPacificMountainWest South Ce...   \n",
      "18  East South CentralPacificMountainWest South Ce...   \n",
      "19  East South CentralPacificMountainWest South Ce...   \n",
      "20  East South CentralPacificMountainWest South Ce...   \n",
      "21  East South CentralPacificMountainWest South Ce...   \n",
      "22  East South CentralPacificMountainWest South Ce...   \n",
      "23  East South CentralPacificMountainWest South Ce...   \n",
      "24  East South CentralPacificMountainWest South Ce...   \n",
      "25  East South CentralPacificMountainWest South Ce...   \n",
      "26  East South CentralPacificMountainWest South Ce...   \n",
      "27  East South CentralPacificMountainWest South Ce...   \n",
      "28  East South CentralPacificMountainWest South Ce...   \n",
      "29  East South CentralPacificMountainWest South Ce...   \n",
      "30  East South CentralPacificMountainWest South Ce...   \n",
      "31  East South CentralPacificMountainWest South Ce...   \n",
      "32  East South CentralPacificMountainWest South Ce...   \n",
      "33  East South CentralPacificMountainWest South Ce...   \n",
      "34  East South CentralPacificMountainWest South Ce...   \n",
      "35  East South CentralPacificMountainWest South Ce...   \n",
      "36  East South CentralPacificMountainWest South Ce...   \n",
      "37  East South CentralPacificMountainWest South Ce...   \n",
      "38  East South CentralPacificMountainWest South Ce...   \n",
      "39  East South CentralPacificMountainWest South Ce...   \n",
      "40  East South CentralPacificMountainWest South Ce...   \n",
      "41  East South CentralPacificMountainWest South Ce...   \n",
      "42  East South CentralPacificMountainWest South Ce...   \n",
      "43  East South CentralPacificMountainWest South Ce...   \n",
      "44  East South CentralPacificMountainWest South Ce...   \n",
      "45  East South CentralPacificMountainWest South Ce...   \n",
      "46  East South CentralPacificMountainWest South Ce...   \n",
      "47  East South CentralPacificMountainWest South Ce...   \n",
      "48  East South CentralPacificMountainWest South Ce...   \n",
      "49  East South CentralPacificMountainWest South Ce...   \n",
      "50  East South CentralPacificMountainWest South Ce...   \n",
      "\n",
      "                                                state  individuals  \\\n",
      "0                                             Alabama       2570.0   \n",
      "1                                       AlabamaAlaska       4004.0   \n",
      "2                                AlabamaAlaskaArizona      11263.0   \n",
      "3                        AlabamaAlaskaArizonaArkansas      13543.0   \n",
      "4              AlabamaAlaskaArizonaArkansasCalifornia     122551.0   \n",
      "5      AlabamaAlaskaArizonaArkansasCaliforniaColorado     130158.0   \n",
      "6   AlabamaAlaskaArizonaArkansasCaliforniaColorado...     132438.0   \n",
      "7   AlabamaAlaskaArizonaArkansasCaliforniaColorado...     133146.0   \n",
      "8   AlabamaAlaskaArizonaArkansasCaliforniaColorado...     136916.0   \n",
      "9   AlabamaAlaskaArizonaArkansasCaliforniaColorado...     158359.0   \n",
      "10  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     165302.0   \n",
      "11  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     169433.0   \n",
      "12  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     170730.0   \n",
      "13  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     177482.0   \n",
      "14  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     181258.0   \n",
      "15  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     182969.0   \n",
      "16  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     184412.0   \n",
      "17  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     187147.0   \n",
      "18  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     189687.0   \n",
      "19  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     191137.0   \n",
      "20  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     196051.0   \n",
      "21  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     202862.0   \n",
      "22  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     208071.0   \n",
      "23  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     212064.0   \n",
      "24  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     213088.0   \n",
      "25  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     216864.0   \n",
      "26  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     217847.0   \n",
      "27  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     219592.0   \n",
      "28  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     226650.0   \n",
      "29  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     227485.0   \n",
      "30  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     233533.0   \n",
      "31  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     235482.0   \n",
      "32  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     275309.0   \n",
      "33  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     281760.0   \n",
      "34  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     282227.0   \n",
      "35  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     289156.0   \n",
      "36  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     291979.0   \n",
      "37  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     303118.0   \n",
      "38  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     311281.0   \n",
      "39  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     312028.0   \n",
      "40  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     315110.0   \n",
      "41  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     315946.0   \n",
      "42  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     322085.0   \n",
      "43  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     341284.0   \n",
      "44  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     343188.0   \n",
      "45  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     343968.0   \n",
      "46  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     347896.0   \n",
      "47  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     364320.0   \n",
      "48  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     365341.0   \n",
      "49  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     368081.0   \n",
      "50  AlabamaAlaskaArizonaArkansasCaliforniaColorado...     368515.0   \n",
      "\n",
      "    family_members  state_pop  \n",
      "0            864.0    4887681  \n",
      "1           1446.0    5622820  \n",
      "2           4052.0   12780844  \n",
      "3           4484.0   15790577  \n",
      "4          25448.0   55252165  \n",
      "5          28698.0   60943452  \n",
      "6          30394.0   64514972  \n",
      "7          30768.0   65480451  \n",
      "8          33902.0   66181998  \n",
      "9          43489.0   87426315  \n",
      "10         46045.0   97937446  \n",
      "11         48444.0   99358039  \n",
      "12         49159.0  101108575  \n",
      "13         53050.0  113831646  \n",
      "14         54532.0  120527143  \n",
      "15         55570.0  123675761  \n",
      "16         56343.0  126587120  \n",
      "17         57296.0  131048273  \n",
      "18         57815.0  135707963  \n",
      "19         58881.0  137047020  \n",
      "20         61111.0  143082822  \n",
      "21         74368.0  149965457  \n",
      "22         77510.0  159949529  \n",
      "23         80760.0  165555778  \n",
      "24         81088.0  168536798  \n",
      "25         83195.0  174658421  \n",
      "26         83617.0  175719086  \n",
      "27         84293.0  177644700  \n",
      "28         84779.0  180672041  \n",
      "29         85394.0  182025506  \n",
      "30         88744.0  190911531  \n",
      "31         89346.0  193004272  \n",
      "32        141416.0  212534623  \n",
      "33        144233.0  222916238  \n",
      "34        144308.0  223674318  \n",
      "35        147628.0  235350659  \n",
      "36        148676.0  239290894  \n",
      "37        152013.0  243472780  \n",
      "38        157362.0  256273702  \n",
      "39        157716.0  257331989  \n",
      "40        158567.0  262416145  \n",
      "41        158890.0  263294843  \n",
      "42        160634.0  270066474  \n",
      "43        166745.0  298695140  \n",
      "44        167717.0  301848690  \n",
      "45        168228.0  302473048  \n",
      "46        170275.0  310974334  \n",
      "47        176155.0  318498203  \n",
      "48        176377.0  320302494  \n",
      "49        178544.0  326109900  \n",
      "50        178749.0  326687501  \n"
     ]
    }
   ],
   "source": [
    "#commulative stats\n",
    "print(homelessness_df.cumsum()) #commulative sum\n",
    "#print(homelessness_df.cummax()) #commulative max\n",
    "#print(homelessness_df.cummin()) #commulative min\n",
    "#print(homelessness_df.cumprod()) #commulative product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3267f1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abidjan       165\n",
      "Mogadishu     165\n",
      "Peking        165\n",
      "Paris         165\n",
      "New York      165\n",
      "             ... \n",
      "Fortaleza     165\n",
      "Faisalabad    165\n",
      "Durban        165\n",
      "Dhaka         165\n",
      "Xian          165\n",
      "Name: city, Length: 100, dtype: int64\n",
      "__________________________________________________________________________\n",
      "Abidjan       0.01\n",
      "Mogadishu     0.01\n",
      "Peking        0.01\n",
      "Paris         0.01\n",
      "New York      0.01\n",
      "              ... \n",
      "Fortaleza     0.01\n",
      "Faisalabad    0.01\n",
      "Durban        0.01\n",
      "Dhaka         0.01\n",
      "Xian          0.01\n",
      "Name: city, Length: 100, dtype: float64\n",
      "__________________________________________________________________________\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10774 entries, 0 to 10773\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   store                 10774 non-null  int64  \n",
      " 1   type                  10774 non-null  object \n",
      " 2   department            10774 non-null  int64  \n",
      " 3   date                  10774 non-null  object \n",
      " 4   weekly_sales          10774 non-null  float64\n",
      " 5   is_holiday            10774 non-null  bool   \n",
      " 6   temperature_c         10774 non-null  float64\n",
      " 7   fuel_price_usd_per_l  10774 non-null  float64\n",
      " 8   unemployment          10774 non-null  float64\n",
      "dtypes: bool(1), float64(4), int64(2), object(2)\n",
      "memory usage: 768.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#categorical variable stat\n",
    "#homelessness_df\n",
    "homelessness_df=homelessness_df.drop_duplicates(subset=[\"region\",\"state\"]) #dropped duplicates from homelessness_df of column region and state\n",
    "#homelessness_df\n",
    "\n",
    "#temperature_df\n",
    "print(temperature_df[\"city\"].value_counts(sort=True)) #city main maaujood jitne bhi sheher hain unki count descending order order main\n",
    "print(\"__________________________________________________________________________\")\n",
    "print(temperature_df[\"city\"].value_counts(normalize=True)) \n",
    "print(\"__________________________________________________________________________\")\n",
    "print(sales_subset_df.info()) #datatype of each column, show summary of different datatypes used in tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3e3f69d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        2.492450e+04\n",
      "1        4.675240e+04\n",
      "2        1.040108e+05\n",
      "3        1.214248e+05\n",
      "4        1.389829e+05\n",
      "             ...     \n",
      "10769    2.568930e+08\n",
      "10770    2.568934e+08\n",
      "10771    2.568938e+08\n",
      "10772    2.568938e+08\n",
      "10773    2.568947e+08\n",
      "Name: weekly_sales, Length: 10774, dtype: float64\n",
      "store  type  department  date        weekly_sales  is_holiday  temperature_c  fuel_price_usd_per_l  unemployment\n",
      "1      A     1           2010-02-05  24924.50      False       5.727778       0.679451              8.106           1\n",
      "19     A     98          2010-12-03  11666.48      False       2.577778       0.845351              8.067           1\n",
      "                         2010-04-02  15413.55      False       7.366667       0.781157              8.185           1\n",
      "                         2010-05-07  13716.77      False       16.966667      0.817613              8.185           1\n",
      "                         2010-06-04  13482.89      False       20.488889      0.794102              8.185           1\n",
      "                                                                                                                   ..\n",
      "6      A     99          2012-10-05  440.00        False       21.577778      0.955511              5.329           1\n",
      "10     B     1           2010-02-05  40212.84      False       12.411111      0.782478              9.765           1\n",
      "                         2010-03-05  36572.44      False       13.288889      0.760023              9.765           1\n",
      "                         2010-04-02  131853.01     False       17.588889      0.815235              9.524           1\n",
      "39     A     99          2012-10-05  915.00        False       22.250000      0.955511              6.228           1\n",
      "Length: 10774, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(sales_subset_df[\"weekly_sales\"].cumsum()) #cummulative calculation\n",
    "#sales_subset_df=sales_subset_df.drop_duplicates(subset=[\"type\"]) #drop duplicates\n",
    "print(sales_subset_df.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e251d",
   "metadata": {},
   "source": [
    "<h1>Grouped Summary</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9d190bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>type</th>\n",
       "      <th>department</th>\n",
       "      <th>date</th>\n",
       "      <th>weekly_sales</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>temperature_c</th>\n",
       "      <th>fuel_price_usd_per_l</th>\n",
       "      <th>unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>False</td>\n",
       "      <td>5.727778</td>\n",
       "      <td>0.679451</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>21827.90</td>\n",
       "      <td>False</td>\n",
       "      <td>8.055556</td>\n",
       "      <td>0.693452</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-04-02</td>\n",
       "      <td>57258.43</td>\n",
       "      <td>False</td>\n",
       "      <td>16.816667</td>\n",
       "      <td>0.718284</td>\n",
       "      <td>7.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-05-07</td>\n",
       "      <td>17413.94</td>\n",
       "      <td>False</td>\n",
       "      <td>22.527778</td>\n",
       "      <td>0.748928</td>\n",
       "      <td>7.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-06-04</td>\n",
       "      <td>17558.09</td>\n",
       "      <td>False</td>\n",
       "      <td>27.050000</td>\n",
       "      <td>0.714586</td>\n",
       "      <td>7.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10769</th>\n",
       "      <td>39</td>\n",
       "      <td>A</td>\n",
       "      <td>99</td>\n",
       "      <td>2011-12-09</td>\n",
       "      <td>895.00</td>\n",
       "      <td>False</td>\n",
       "      <td>9.644444</td>\n",
       "      <td>0.834256</td>\n",
       "      <td>7.716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10770</th>\n",
       "      <td>39</td>\n",
       "      <td>A</td>\n",
       "      <td>99</td>\n",
       "      <td>2012-02-03</td>\n",
       "      <td>350.00</td>\n",
       "      <td>False</td>\n",
       "      <td>15.938889</td>\n",
       "      <td>0.887619</td>\n",
       "      <td>7.244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10771</th>\n",
       "      <td>39</td>\n",
       "      <td>A</td>\n",
       "      <td>99</td>\n",
       "      <td>2012-06-08</td>\n",
       "      <td>450.00</td>\n",
       "      <td>False</td>\n",
       "      <td>27.288889</td>\n",
       "      <td>0.911922</td>\n",
       "      <td>6.989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10772</th>\n",
       "      <td>39</td>\n",
       "      <td>A</td>\n",
       "      <td>99</td>\n",
       "      <td>2012-07-13</td>\n",
       "      <td>0.06</td>\n",
       "      <td>False</td>\n",
       "      <td>25.644444</td>\n",
       "      <td>0.860145</td>\n",
       "      <td>6.623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10773</th>\n",
       "      <td>39</td>\n",
       "      <td>A</td>\n",
       "      <td>99</td>\n",
       "      <td>2012-10-05</td>\n",
       "      <td>915.00</td>\n",
       "      <td>False</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>0.955511</td>\n",
       "      <td>6.228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10774 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       store type  department        date  weekly_sales  is_holiday  \\\n",
       "0          1    A           1  2010-02-05      24924.50       False   \n",
       "1          1    A           1  2010-03-05      21827.90       False   \n",
       "2          1    A           1  2010-04-02      57258.43       False   \n",
       "3          1    A           1  2010-05-07      17413.94       False   \n",
       "4          1    A           1  2010-06-04      17558.09       False   \n",
       "...      ...  ...         ...         ...           ...         ...   \n",
       "10769     39    A          99  2011-12-09        895.00       False   \n",
       "10770     39    A          99  2012-02-03        350.00       False   \n",
       "10771     39    A          99  2012-06-08        450.00       False   \n",
       "10772     39    A          99  2012-07-13          0.06       False   \n",
       "10773     39    A          99  2012-10-05        915.00       False   \n",
       "\n",
       "       temperature_c  fuel_price_usd_per_l  unemployment  \n",
       "0           5.727778              0.679451         8.106  \n",
       "1           8.055556              0.693452         8.106  \n",
       "2          16.816667              0.718284         7.808  \n",
       "3          22.527778              0.748928         7.808  \n",
       "4          27.050000              0.714586         7.808  \n",
       "...              ...                   ...           ...  \n",
       "10769       9.644444              0.834256         7.716  \n",
       "10770      15.938889              0.887619         7.244  \n",
       "10771      27.288889              0.911922         6.989  \n",
       "10772      25.644444              0.860145         6.623  \n",
       "10773      22.250000              0.955511         6.228  \n",
       "\n",
       "[10774 rows x 9 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To apply the statistics operations on a numeric column for categorical column\n",
    "#that can be done by groupby summary\n",
    "sales_subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f17ca67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A' 'B']\n",
      "__________________________________________________________________________________\n",
      "type\n",
      "A    23674.667242\n",
      "B    25696.678370\n",
      "Name: weekly_sales, dtype: float64\n",
      "__________________________________________________________________________________\n",
      "         min        max           sum\n",
      "type                                 \n",
      "A    -1098.0  293966.05  2.337163e+08\n",
      "B     -798.0  232558.51  2.317840e+07\n",
      "__________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(sales_subset_df[\"type\"].unique())\n",
    "print(\"__________________________________________________________________________________\")\n",
    "#calculate weekly sales of store a and b mean,median,max, and sum\n",
    "print(sales_subset_df.groupby(\"type\")[\"weekly_sales\"].mean())\n",
    "print(\"__________________________________________________________________________________\")\n",
    "print(sales_subset_df.groupby(\"type\")[\"weekly_sales\"].agg([min,max,sum]))\n",
    "print(\"__________________________________________________________________________________\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f7159d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'Timestamp' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2680\\614423251.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m'Unknown'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#create a dataframe of categorized dates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0msales_subset_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"year\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msales_subset_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"date\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategorize_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mG:\\DICE ANALYTICS\\ANNACONDA SETUP\\ANA\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4431\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4432\u001b[0m         \"\"\"\n\u001b[1;32m-> 4433\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4435\u001b[0m     def _reduce(\n",
      "\u001b[1;32mG:\\DICE ANALYTICS\\ANNACONDA SETUP\\ANA\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1086\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1087\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1088\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1089\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\DICE ANALYTICS\\ANNACONDA SETUP\\ANA\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1141\u001b[0m                 \u001b[1;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m                 \u001b[1;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1143\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1144\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\DICE ANALYTICS\\ANNACONDA SETUP\\ANA\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2680\\614423251.py\u001b[0m in \u001b[0;36mcategorize_date\u001b[1;34m(date)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcategorize_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend_date\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcategory\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdate_ranges\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mstart_date\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mend_date\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m'Unknown'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<=' not supported between instances of 'Timestamp' and 'str'"
     ]
    }
   ],
   "source": [
    "#to calculate the weekly sales\n",
    "\n",
    "#the first issue is we dont have the categorical variable that defines the dates\n",
    "\n",
    "#first we have to create dates categorical variable and this is done under the umbrella of feature engineering\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#step#01:craete date ranges\n",
    "date_ranges=[\n",
    "    (pd.Timestamp(\"2010-01-01\"),pd.Timestamp(\"2010-12-31\"),\"2010\"),\n",
    "    (pd.Timestamp(\"2011-01-01\"),pd.Timestamp(\"2011-12-31\"),\"2011\"),\n",
    "    (pd.Timestamp(\"2012-01-01\"),pd.Timestamp(\"2012-12-31\"),\"2012\")\n",
    "]\n",
    "#step#02:Create a function to categorize the dates based on the defined ranges\n",
    "def categorize_date(date):\n",
    "    for start_date,end_date,category in date_ranges:\n",
    "        if start_date <= date <= end_date:\n",
    "            return category\n",
    "    return 'Unknown'\n",
    "#create a dataframe of categorized dates\n",
    "sales_subset_df[\"year\"]=sales_subset_df[\"date\"].apply(categorize_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267fdb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped summary\n",
    "sales_subset_df\n",
    "temperature_df\n",
    "homelessness_df\n",
    "avocado_df\n",
    "#type aur year ke hisaab se sales batao\n",
    "\n",
    "\n",
    "type_year_nbsold = avocado_df.groupby([\"type\", \"year\"])[\"nb_sold\"].agg(\n",
    "    mean='mean',\n",
    "    total='sum',\n",
    "    minimum='min',\n",
    "    maximum='max'\n",
    ")\n",
    "\n",
    "type_year_nbsold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a36a35a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1014 entries, 0 to 1013\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   date       1014 non-null   object \n",
      " 1   type       1014 non-null   object \n",
      " 2   year       1014 non-null   int64  \n",
      " 3   avg_price  1014 non-null   float64\n",
      " 4   size       1014 non-null   object \n",
      " 5   nb_sold    1014 non-null   float64\n",
      "dtypes: float64(2), int64(1), object(3)\n",
      "memory usage: 47.7+ KB\n",
      "                 mean\n",
      "            avg_price\n",
      "size                 \n",
      "extra_large  1.319024\n",
      "large        1.319024\n",
      "small        1.319024\n",
      "                     mean                      median           \n",
      "type         conventional        organic conventional    organic\n",
      "size                                                            \n",
      "extra_large  9.187040e+05    5409.545503    821964.42    4880.83\n",
      "large        1.161486e+07  308284.424201  11354281.64  287724.61\n",
      "small        1.201098e+07  148404.667515  12091315.22  132040.04\n",
      "0\n",
      "nb_sold      0\n",
      "avg_price    0\n",
      "dtype: int64\n",
      "size       extra_large        large        small\n",
      "avg_price                                       \n",
      "0.76        2546439.11  20470572.61  16573573.78\n",
      "0.77        1664383.09  20328161.55  22743616.17\n",
      "0.82        1993645.36  17896391.60  14223304.98\n",
      "0.84        1414364.49  15899858.37  13748944.38\n",
      "0.86        1560068.62  16054083.86  11616506.17\n",
      "...                ...          ...          ...\n",
      "1.96           1154.19    262747.80    108658.66\n",
      "2.00            891.60    243233.57    140979.17\n",
      "2.03            940.89    249231.14    114942.21\n",
      "2.06           2256.96    256920.07    113516.00\n",
      "2.09           2119.14    250861.84    112812.39\n",
      "\n",
      "[109 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#pivot tables\n",
    "#aik categorical variable ke hisaab se aik numerical variable calculate kro\n",
    "avocado_df.columns\n",
    "avocado_df.info()\n",
    "print(avocado_df.pivot_table(values=\"avg_price\",index=\"size\",aggfunc=[np.mean]))\n",
    "#2 categorical variable aur aik numerical\n",
    "print(avocado_df.pivot_table(values=\"nb_sold\",index=\"size\",columns=\"type\",aggfunc=[np.mean,np.median]))\n",
    "#main advantage of using pivot table is we can fill 0 to na values\n",
    "print(avocado_df[\"nb_sold\"].isnull().sum()) #tocalculate missing values in a column\n",
    "print(avocado_df.loc[:,[\"nb_sold\",\"avg_price\"]].isnull().sum()) #to calculate missing values of multiple columns\n",
    "\n",
    "print(avocado_df.pivot_table(values=\"nb_sold\",index=\"avg_price\",columns=\"size\",fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e666f6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 country    capital    area\n",
      "population                                 \n",
      "200.40            Brazil   Brasilia   8.516\n",
      "143.50            Russia     Moscow  17.100\n",
      "1252.00            India  New Delhi   3.286\n",
      "1357.00            China    Beijing   9.597\n",
      "52.98       South Africa   Pretoria   1.221\n",
      "                            date          type  year         size\n",
      "nb_sold    avg_price                                             \n",
      "9626901.09 0.95       2015-12-27  conventional  2015        small\n",
      "8710021.76 0.98       2015-12-20  conventional  2015        small\n",
      "9855053.66 0.93       2015-12-13  conventional  2015        small\n",
      "9405464.36 0.89       2015-12-06  conventional  2015        small\n",
      "8094803.56 0.99       2015-11-29  conventional  2015        small\n",
      "...                          ...           ...   ...          ...\n",
      "1703.52    1.53       2018-02-04       organic  2018  extra_large\n",
      "1270.61    1.61       2018-01-28       organic  2018  extra_large\n",
      "1490.02    1.63       2018-01-21       organic  2018  extra_large\n",
      "1580.01    1.59       2018-01-14       organic  2018  extra_large\n",
      "1289.07    1.51       2018-01-07       organic  2018  extra_large\n",
      "\n",
      "[1014 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#to reset and replace an index\n",
    "avocado_df=avocado_df.reset_index(drop=True)\n",
    "avocado_df\n",
    "#replaced_index_avocado=avocado_df.reset_index([\"names_of_indexes\"])\n",
    "#apply on brics\n",
    "brics_df\n",
    "#repl_ind_brics=brics_df.reset_index([\"BRAZIL\",\"RUSSIA\",\"INDIA\",\"CHINA\",\"SOUTH_AFRICA\"])\n",
    "#print(brics_df[repl_ind_brics])\n",
    "#failed: to reset an index\n",
    "\n",
    "#make column to index\n",
    "subset_index=brics_df.set_index(\"population\") #how to make column row index\n",
    "print(subset_index)\n",
    "\n",
    "#how to make 2 columns row index\n",
    "avocado_df\n",
    "two_columns_to_row_index_avocado=avocado_df.set_index([\"nb_sold\",\"avg_price\"])\n",
    "print(two_columns_to_row_index_avocado)\n",
    "\n",
    "#index based operations violates tidy data principles then leave it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aab98502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store                      15.441897\n",
      "department                 45.218118\n",
      "weekly_sales            23843.950149\n",
      "is_holiday                  0.003898\n",
      "temperature_c              15.731978\n",
      "fuel_price_usd_per_l        0.749746\n",
      "unemployment                8.082009\n",
      "dtype: float64\n",
      "0        3563.001890\n",
      "1        3120.965001\n",
      "2        8183.681850\n",
      "3        2492.432101\n",
      "4        2513.666084\n",
      "            ...     \n",
      "10769     150.170671\n",
      "10770      73.152930\n",
      "10771      89.027116\n",
      "10772      24.455370\n",
      "10773     154.633359\n",
      "Length: 10774, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HR Computers\\AppData\\Local\\Temp\\ipykernel_2680\\1155794009.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  print(sales_subset_df.mean(axis=\"index\")) #adding row by row so column entity calculated\n",
      "C:\\Users\\HR Computers\\AppData\\Local\\Temp\\ipykernel_2680\\1155794009.py:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  print(sales_subset_df.mean(axis=\"columns\")) #adding column by column so row index entity calculated\n"
     ]
    }
   ],
   "source": [
    "#to calculate mean across row and columns\n",
    "print(sales_subset_df.mean(axis=\"index\")) #adding row by row so column entity calculated\n",
    "print(sales_subset_df.mean(axis=\"columns\")) #adding column by column so row index entity calculated\n",
    "\n",
    "#by the way useful is row entity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
